{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping data from PubMed using Selenium and BeautifulSoup\n",
    "\n",
    "In this notebook I will be scraping data from [PubMed](https://pubmed.ncbi.nlm.nih.gov/about/) - a free resource for search and retrieval of medical and life sciences literature. This is usually the first stop for a researcher interested in a specific topic. The aim is to collect as much information as possible about articles that show up in the query results of a specific topic. \n",
    "\n",
    "My topic of choosing is 'stem cell therapies'. I specifically chose this topic because I have been working in a biotechnology company that supplies materials for stem cell researchers. The products I have been working on are for Pulmonary (lung) researchers. I wanted to use this opportunity to step back and understand what the field looks like from afar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 'Stem Cell Therapies' Using Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Selenium](https://www.selenium.dev/documentation/en/) is a tool to automate web browsing. You can open a browser, go to a specific url and type and click away. This is me figuring out the basics by opening PubMed's home page and entering the search term 'stem cell therapies':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Connect to pubmed\n",
    "url = 'https://pubmed.ncbi.nlm.nih.gov/'\n",
    "driver.get(url)\n",
    "\n",
    "# Locate the search bar from html\n",
    "search_bar = driver.find_element_by_xpath('//*[@id=\"id_term\"]')\n",
    "\n",
    "# Input search term into the search bar\n",
    "search_term = 'stem cell therapies'\n",
    "search_bar.send_keys(search_term) # I can type into the search bar!\n",
    "search_bar.send_keys(Keys.RETURN) # And press ENTER to search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search result\n",
    "Scrolling to the bottom of the page, we see that there are 218,709 results (publications) about 'stem cell therapies'! \n",
    "\n",
    "\n",
    "The default setting for PubMed sorts the results by 'Best match'. Each result is shown with a linked title, authors, journal title and doi, PMID (PubMed's internal ID for the publication), publication type, and a snippet of its abstract. The titles are linked to a page within PubMed with more comprehensive information about the publication. \n",
    "\n",
    "\n",
    "One can scroll down and press `Show more` to show more pages of the search results, or add `&page=` and the desired page number at the end of the url. Each page contains 10 publications. The `Jump to page` link shows that there are 21,871 pages, however, PubMed limits the maximum value of this to 1000. So we will only be able to scrape 1000 pages of 10 articles (10000 articles) at most. \n",
    "\n",
    "The link to the PubMed page for each article follows this pattern: `https://pubmed.ncbi.nlm.nih.gov/` followed by the publication's PMID. There, one can find the article title, a comprehensive list of authors, the affiliations each author has, PMID, DOI, the full abstract, short form of the journal title, and date of publication. Towards the bottom, there are links to similar articles, the list of articles that cited the publication, MeSH terms (keywords) associated with the publication, and the number of references the publication used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping using Selenium and BeautifulSoup:\n",
    "\n",
    "Selenium is great for controlling the browser, but not as helpful for extracting information about each web page that I've identified above. For this, I will use BeautifulSoup which is fantastic at extracting information from the web page's HTML code. \n",
    "\n",
    "I will loop through the pages (1 through 1000) and at each page, I will scrape the PMIDs for each publication. Then looping through each PMID, I will scrape the key information as noted above for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define arguments\n",
    "pages = np.arange(1, 1001, 1)\n",
    "search_term = 'stem cell therapy'\n",
    "search_term = search_term.replace(' ', '%20')\n",
    "\n",
    "# Define source and destination paths\n",
    "source = f'~/Downloads/PubMed_Timeline_Results_by_Year.csv'\n",
    "destination = f'C:~/Desktop/{search_term}_npubs.csv'\n",
    "\n",
    "# Create empty dictionary to store article information\n",
    "article_dict = {'article_id':[],\n",
    "                'title':[],\n",
    "                'publication_type':[],\n",
    "                'abstract':[],\n",
    "                'journal_title':[],\n",
    "                'citation':[],\n",
    "                'n_authors':[],\n",
    "                'affiliations':[],\n",
    "                'n_affiliations':[],\n",
    "                'n_citations':[],\n",
    "                'keywords':[],\n",
    "                'n_references':[]}\n",
    "\n",
    "# Create empty dictionary for error articles\n",
    "error_articles = []\n",
    "\n",
    "# Loop through pages to obtain article_ids pertaining to search term\n",
    "for p in pages:\n",
    "    \n",
    "    # Create empty list to store all article ids\n",
    "    article_ids = []\n",
    "    \n",
    "    try:\n",
    "        # Define url\n",
    "        url = f'https://pubmed.ncbi.nlm.nih.gov/?term={search_term}&page={p}'\n",
    "        \n",
    "        # Use chromedriver to open url\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Download csv for n_publications over the years\n",
    "        if p == 1:\n",
    "            # Find download csv button and click\n",
    "            n_pubs = driver.find_element_by_xpath('//*[@id=\"side-download-results-by-year-button\"]')\n",
    "            n_pubs.click()\n",
    "            # Wait for download\n",
    "            time.sleep(2)\n",
    "            # Move into data folder in project folder\n",
    "            shutil.move(source, destination)\n",
    "        \n",
    "        # Make html soup of the page\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Create a list of the search results\n",
    "        docsum = soup.find_all('a', class_='docsum-title')\n",
    "        \n",
    "        # Extract only the article id numbers and append to article_ids\n",
    "        for i in range(len(docsum)):\n",
    "            article_ids.append(docsum[i]['data-article-id'])\n",
    "\n",
    "        # Status update\n",
    "        print(f'{len(article_ids)} Articles found on Page {p}...                    ', end='\\r')                       \n",
    "            \n",
    "        # Wait\n",
    "        time.sleep(random.randint(0, 5)/10)\n",
    "        \n",
    "        # Loop through each article page\n",
    "        for i in range(len(article_ids)):\n",
    "    \n",
    "            try:\n",
    "    \n",
    "                # Set url\n",
    "                url = f'https://pubmed.ncbi.nlm.nih.gov/{article_ids[i]}/'\n",
    "    \n",
    "                # Set driver\n",
    "                driver = webdriver.Chrome()\n",
    "                driver.get(url)\n",
    "    \n",
    "                # Make html soup\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "                # Get article information\n",
    "                # Title\n",
    "                try:\n",
    "                    title = soup.find('title').text\n",
    "                except:\n",
    "                    title = ''\n",
    "                # Publication Type\n",
    "                try:\n",
    "                    pub_type = soup.find(class_='publication-type').text\n",
    "                except:\n",
    "                    pub_type = ''\n",
    "                # Abstract\n",
    "                try:\n",
    "                    abstract = soup.find(class_=\"abstract-content selected\").text\n",
    "                except:\n",
    "                    abstract = ''\n",
    "                # Journal Title\n",
    "                journal_info = soup.find(class_='journal-actions dropdown-block')\n",
    "                journal_title = journal_info.find('button')['title']\n",
    "                # Citation for Date\n",
    "                try:\n",
    "                    citation = soup.find('span', class_='cit').text\n",
    "                except:\n",
    "                    citation = ''\n",
    "                # Authors Info\n",
    "                authors_info = soup.find('div', class_='authors-list')\n",
    "                authors = authors_info.find_all('a', class_='full-name')\n",
    "                n_authors = len(authors)\n",
    "                # Affiliation Info (for institution)\n",
    "                affs = authors_info.find_all('a', class_='affiliation-link')\n",
    "                affiliations = [aff['title'] for aff in affs]\n",
    "                n_affiliations = len(affiliations)\n",
    "                # Number of citations\n",
    "                try:\n",
    "                    cited_by = soup.find('em', class_='amount').text\n",
    "                except:\n",
    "                    cited_by = ''\n",
    "                # Keywords deemed by Pubmed\n",
    "                s = str(soup.find('div', class_='mesh-terms keywords-section'))\n",
    "                keywords_long = re.findall(r'Toggle dropdown menu for keyword [\\w\\s /\\*]+', s)\n",
    "                keywords = [keyword.lstrip('Toggle dropdown menu for keyword ') for keyword in keywords_long]\n",
    "                # Number of References\n",
    "                try:\n",
    "                    n_refs = soup.find('div', class_='refs-list').find('button').text\n",
    "                except:\n",
    "                    n_refs = ''\n",
    "    \n",
    "                # Add info into article_dict\n",
    "                article_dict['article_id'].append(article_ids[i])\n",
    "                article_dict['title'].append(title)\n",
    "                article_dict['publication_type'].append(pub_type)\n",
    "                article_dict['abstract'].append(abstract)\n",
    "                article_dict['journal_title'].append(journal_title)\n",
    "                article_dict['citation'].append(citation)\n",
    "                article_dict['n_authors'].append(n_authors)\n",
    "                article_dict['affiliations'].append(affiliations)\n",
    "                article_dict['n_affiliations'].append(n_affiliations)\n",
    "                article_dict['n_citations'].append(cited_by)\n",
    "                article_dict['keywords'].append(keywords)\n",
    "                article_dict['n_references'].append(n_refs)\n",
    "        \n",
    "                # Status update\n",
    "                print(f'Currently on Page {p} / {max(pages)}: Article {i+1} / {len(article_ids)} done          ', end='\\r')\n",
    "        \n",
    "                # Wait\n",
    "                time.sleep(random.randint(0, 5)/10)\n",
    "    \n",
    "            # Exception message\n",
    "            except Exception as ex:\n",
    "                template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "                message = template.format(type(ex).__name__, ex.args)\n",
    "                print(message, f'Page {p}, Article {i+1}')\n",
    "                \n",
    "                # Delete entries of this loop if problem arises\n",
    "                for k in article_dict.keys():\n",
    "                    if len(article_dict[k]) == i+1:\n",
    "                        article_dict[k].pop()\n",
    "                \n",
    "                error_articles.append(article_ids[i])\n",
    "        \n",
    "                continue\n",
    "        \n",
    "        # Status update\n",
    "        print(f'Page {p} / {len(pages)} done          ', end='\\r')\n",
    "        \n",
    "        # Wait\n",
    "        time.sleep(random.randint(0, 5)/10)\n",
    "        \n",
    "    except Exception as ex:\n",
    "        template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "        message = template.format(type(ex).__name__, ex.args)\n",
    "        print(message, f'Page {p}')\n",
    "        # https://stackoverflow.com/questions/9823936/python-how-do-i-know-what-type-of-exception-occurred\n",
    "        \n",
    "print('All done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9926, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe from results and look at it's shape\n",
    "pd.DataFrame(article_dict).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was able to collect information on 99.26% of the publications that I crawled in 14.5 hours. The errors are most likely due to articles lacking information about authors, affiliations, or keywords as I forgot to add a try/except message there. I can check this in the future from the `error_articles` list. \n",
    "\n",
    "The scrape took 14.5 hours. Although I was able to do things in the meantime (beauty of automation!), I hope there is a way to cut this time down. For now, I will not trouble shoot this any further so that I can move on with my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results into a csv\n",
    "pd.DataFrame(article_dict).to_csv('data/pubmedscrape_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save error pages into a csv\n",
    "pd.DataFrame({'errors':error_articles}).to_csv('error_articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few things to note:\n",
    "\n",
    "- Status update message can be improved\n",
    "- Would be helpful to print `x% of job done` type of message\n",
    "- Could really use helper functions here\n",
    "- Need to add try and except for `keywords`, `authors`, `affiliations`\n",
    "- Would love to compile this into a script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Using Selenium and BeautifulSoup I was able to automate data collection and compile information from 9926 publications about 'stem cell therapies' from PubMed. The current method is not perfect as it does not make use of helper functions or scripting and will need to be improved on in the future for more efficiency."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
